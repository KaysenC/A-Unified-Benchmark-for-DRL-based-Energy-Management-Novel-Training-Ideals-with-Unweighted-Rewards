# A-Unified-Benchmark-for-DRL-based-Energy-Management-Novel-Training-Ideals-with-Unweighted-Rewards

Addressing three problems in the current field of RL-based EMSs for HEVs: the lack of a unified and reliable benchmark model and results, the traditional reward function interfering with the training process of agents caused by unideal optimization terms, and the significant effort, time, and personal expertise required to adjust weight parameters, this paper adopts two Python-based standard DRL agents, DQN and DDPG, and four Simulink-based public hybrid powertrains, P1-P4 applications released by MathWorks to perform the Python/Simulink-based co-simulation training, providing a public, fair, and reproducible benchmark for subsequent work. Then, focusing on the flaws of traditional reward functions that are often used in the previous literature, this paper analyzes the fuel consumption and SOC deviation in terms of range, magnitude, and importance, and the results of the DRL-based SOC maintenance strategy, indicate poor robustness. Moreover, this paper proposes a novel training idea, which can guide the DRL agent to explore the optimal strategy by combining the rule-based ESS and the unweighted reward dedicated to maximizing efficiency and optimizing working points. Finally, with the P2 configuration as the target, the HIL experiment is performed, and the official rule-based and ECMS-based EMSs serve as a benchmark for comparison. Simulink-based models are available at https://ww2.mathworks.cn/help/releases/R2020a/autoblks/ug/hybrid-and-electric-vehicle-reference-application-projects.html, two types of standard DRL algorithm models are available at https://github.com/KaysenC/A-Unified-Benchmark-for-DRL-based-Energy-Management-Novel-Training-Ideals-with-Unweighted-Rewards, and some of the results also are public for examination and comparison by other researchers.
